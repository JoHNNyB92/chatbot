{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tAcIybn__qY-"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1062
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35256,
     "status": "ok",
     "timestamp": 1546602624459,
     "user": {
      "displayName": "Ioannis Avgeros",
      "photoUrl": "",
      "userId": "10653338927563797423"
     },
     "user_tz": -120
    },
    "id": "jem16d3V8FtT",
    "outputId": "d14863d3-1eea-483d-9bf6-315bfef0e947"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorlayer\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/55/2dc51f4a8e772240e63c442de06762ddefd0631399f446b6895be5e2590d/tensorlayer-1.11.1-py2.py3-none-any.whl (316kB)\n",
      "\u001b[K    100% |████████████████████████████████| 317kB 21.5MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scikit-learn<0.21,>=0.19 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (0.20.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy<1.2,>=1.1 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt<1.11,>=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.10.11)\n",
      "Collecting matplotlib<3.1,>=2.2 (from tensorlayer)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/07/16d781df15be30df4acfd536c479268f1208b2dfbc91e9ca5d92c9caf673/matplotlib-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (12.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.9MB 3.1MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: progressbar2<3.39,>=3.38 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (3.38.0)\n",
      "Requirement already satisfied, skipping upgrade: lxml<4.3,>=4.2 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (4.2.5)\n",
      "Collecting scikit-image<0.15,>=0.14 (from tensorlayer)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/90/553120309c53bdfca25c9c50769ae40a538a90c24db8c082468aec898d00/scikit_image-0.14.1-cp36-cp36m-manylinux1_x86_64.whl (25.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 25.3MB 1.7MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: imageio<2.5,>=2.3 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (2.4.1)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<4.29,>=4.23 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (4.28.1)\n",
      "Collecting requests<2.21,>=2.19 (from tensorlayer)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/17/5cbb026005115301a8fb2f9b0e3e8d32313142fe8b617070e7baad20554f/requests-2.20.1-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 22.8MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy<1.16,>=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.14.6)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib<3.1,>=2.2->tensorlayer) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<3.1,>=2.2->tensorlayer) (2.5.3)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib<3.1,>=2.2->tensorlayer)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/a7/88719d132b18300b4369fbffa741841cfd36d1e637e1990f27929945b538/kiwisolver-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (949kB)\n",
      "\u001b[K    100% |████████████████████████████████| 952kB 14.3MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<3.1,>=2.2->tensorlayer) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2<3.39,>=3.38->tensorlayer) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from progressbar2<3.39,>=3.38->tensorlayer) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: dask[array]>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (0.20.2)\n",
      "Collecting pillow>=4.3.0 (from scikit-image<0.15,>=0.14->tensorlayer)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e3/217dfd0834a51418c602c96b110059c477260c7fee898542b100913947cf/Pillow-5.4.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.0MB 8.4MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (2.2)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.8,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<2.21,>=2.19->tensorlayer) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<2.21,>=2.19->tensorlayer) (1.22)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<2.21,>=2.19->tensorlayer) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<2.21,>=2.19->tensorlayer) (2018.11.29)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib<3.1,>=2.2->tensorlayer) (40.6.3)\n",
      "Requirement already satisfied, skipping upgrade: toolz>=0.7.3; extra == \"array\" in /usr/local/lib/python3.6/dist-packages (from dask[array]>=0.9.0->scikit-image<0.15,>=0.14->tensorlayer) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image<0.15,>=0.14->tensorlayer) (4.3.0)\n",
      "\u001b[31myellowbrick 0.9 has requirement matplotlib<3.0,>=1.5.1, but you'll have matplotlib 3.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
      "\u001b[31mgoogle-colab 0.0.1a1 has requirement requests~=2.18.0, but you'll have requests 2.20.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mcufflinks 0.14.6 has requirement plotly>=3.0.0, but you'll have plotly 1.12.12 which is incompatible.\u001b[0m\n",
      "Installing collected packages: kiwisolver, matplotlib, pillow, scikit-image, requests, tensorlayer\n",
      "  Found existing installation: matplotlib 2.1.2\n",
      "    Uninstalling matplotlib-2.1.2:\n",
      "      Successfully uninstalled matplotlib-2.1.2\n",
      "  Found existing installation: Pillow 4.0.0\n",
      "    Uninstalling Pillow-4.0.0:\n",
      "      Successfully uninstalled Pillow-4.0.0\n",
      "  Found existing installation: scikit-image 0.13.1\n",
      "    Uninstalling scikit-image-0.13.1:\n",
      "      Successfully uninstalled scikit-image-0.13.1\n",
      "  Found existing installation: requests 2.18.4\n",
      "    Uninstalling requests-2.18.4:\n",
      "      Successfully uninstalled requests-2.18.4\n",
      "Successfully installed kiwisolver-1.0.1 matplotlib-3.0.2 pillow-5.4.0 requests-2.20.1 scikit-image-0.14.1 tensorlayer-1.11.1\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "!pip install --upgrade tensorlayer\n",
    "from tensorlayer.layers import DenseLayer, EmbeddingInputlayer, Seq2Seq, retrieve_seq_length_op2\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "import tensorlayer as tl\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mU6tg2TD_0Kp"
   },
   "source": [
    "# Load File\n",
    "Loading the two necessary files for creating the chatbot.As we can see with the following print,format of the two files is:\n",
    "\n",
    "\n",
    "*  movie_lines.tsv: scene_id [tab] character_id [tab] character_name [tab] text\n",
    "*  movie_conversations.tsv: character_id [tab] character_id2 [tab] movie_id [tab] scenes_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34181,
     "status": "ok",
     "timestamp": 1546602624731,
     "user": {
      "displayName": "Ioannis Avgeros",
      "photoUrl": "",
      "userId": "10653338927563797423"
     },
     "user_tz": -120
    },
    "id": "lqNhAPWzjDVw",
    "outputId": "5fccf33f-d799-415d-ece8-b549f0156118"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L1045\\tu0\\tm0\\tBIANCA\\tThey do not!', 'L1044\\tu2\\tm0\\tCAMERON\\tThey do to!', 'L985\\tu0\\tm0\\tBIANCA\\tI hope so.', 'L984\\tu2\\tm0\\tCAMERON\\tShe okay?', \"L925\\tu0\\tm0\\tBIANCA\\tLet's go.\"]\n",
      "[\"u0\\tu2\\tm0\\t['L194' 'L195' 'L196' 'L197']\", \"u0\\tu2\\tm0\\t['L198' 'L199']\", \"u0\\tu2\\tm0\\t['L200' 'L201' 'L202' 'L203']\", \"u0\\tu2\\tm0\\t['L204' 'L205' 'L206']\", \"u0\\tu2\\tm0\\t['L207' 'L208']\"]\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "lines = open('movie_lines.tsv', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "conv_lines = open('movie_conversations.tsv', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "print (lines[:5])\n",
    "print (conv_lines[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RFL0m1_EA82S"
   },
   "source": [
    "# Functions for handling the data\n",
    "Here,I keep only the information relevant to the chatbot procedure.\n",
    "\n",
    "\n",
    "*   *load_lines_to_ids*:I load the lines of the conversation , keep the line id [ first element] and the text [last element]\n",
    "* *convs_to_line_ids*: Into the list where the line id's are, we assign the different conversation taking place in line file\n",
    "* *split_questions_answer* :Split the dialogue into questions/answers.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZIyl8Zfgi8RX"
   },
   "outputs": [],
   "source": [
    "def load_lines_to_ids(lines):\n",
    "    id2line = {}\n",
    "    for line in lines:\n",
    "        _line = line.split('\\t')\n",
    "        if len(_line) >= 5:\n",
    "            #Remove ' and \" symbol from movie id.\n",
    "            lineId=_line[0].replace(\"'\",\"\").replace(\"\\\"\",\"\")\n",
    "            id2line[lineId] = ''.join(_line[4:(len(_line))])\n",
    "    return id2line\n",
    "\n",
    "def convs_to_line_ids(conv_lines):     \n",
    "    convs = [ ]\n",
    "    for line in conv_lines[:-1]:\n",
    "      \n",
    "        #To the place where movie id's are stores ( e.g. [ 'x1' 'x2' 'x3']) replace ' and [] symbols\n",
    "        _line = line.split('\\t')[-1][1:-1].replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        convs.append(_line.split(' '))\n",
    "    return convs\n",
    "\n",
    "def split_questions_answer(id2line,convs):\n",
    "    q = []\n",
    "    a = []\n",
    "    for conv in convs:\n",
    "        for i in range(len(conv)-1):\n",
    "            q.append(id2line[conv[i]])\n",
    "            a.append(id2line[conv[i+1]])\n",
    "    return (q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34123,
     "status": "ok",
     "timestamp": 1546602625883,
     "user": {
      "displayName": "Ioannis Avgeros",
      "photoUrl": "",
      "userId": "10653338927563797423"
     },
     "user_tz": -120
    },
    "id": "4w4C543Pjl5Z",
    "outputId": "ca9bdf90-cfa1-4481-f84b-2456947e855e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "-Well I thought we'd start with pronunciation if that's okay with you.\n",
      "-------------------------------------\n",
      "-Well I thought we'd start with pronunciation if that's okay with you.\n",
      "-Not the hacking and gagging and spitting part.  Please.\n",
      "-------------------------------------\n",
      "-Not the hacking and gagging and spitting part.  Please.\n",
      "-Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "-------------------------------------\n",
      "-You're asking me out.  That's so cute. What's your name again?\n",
      "-Forget it.\n",
      "-------------------------------------\n",
      "-No no it's my fault -- we didn't have a proper introduction ---\n",
      "-Cameron.\n",
      "-------------------------------------\n",
      "Length of questions=221616\n",
      "Length of answers=221616\n"
     ]
    }
   ],
   "source": [
    "id2l=load_lines_to_ids(lines)\n",
    "c=convs_to_line_ids(conv_lines)\n",
    "(q,a)=split_questions_answer(id2l,c)\n",
    "\n",
    "for i in range(0,5):\n",
    "    print(\"-\"+str(q[i]))\n",
    "    print(\"-\"+str(a[i]))\n",
    "    print(\"-------------------------------------\")\n",
    "\n",
    "print('Length of questions='+str(len(q)))\n",
    "print('Length of answers='+str(len(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-AAHB_eoDRz-"
   },
   "source": [
    "# Clean text\n",
    "We must clean and lowercase the dataset,before we use it. Abbreviations such as *it's* should be transformed to* it is*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QjU6P_5Sj00d"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Im8y-tGQj4dt"
   },
   "outputs": [],
   "source": [
    "clean_q = []\n",
    "for q_ in q:\n",
    "    clean_q.append(clean_text(q_))   \n",
    "clean_a = []    \n",
    "for a_ in a:\n",
    "    clean_a.append(clean_text(a_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 41963,
     "status": "ok",
     "timestamp": 1546602635716,
     "user": {
      "displayName": "Ioannis Avgeros",
      "photoUrl": "",
      "userId": "10653338927563797423"
     },
     "user_tz": -120
    },
    "id": "79Q-pU49j6Y5",
    "outputId": "d103419b-3630-43fc-defd-da9755adbe90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again\n",
      "-well i thought we would start with pronunciation if that is okay with you\n",
      "-------------------------------------\n",
      "-well i thought we would start with pronunciation if that is okay with you\n",
      "-not the hacking and gagging and spitting part  please\n",
      "-------------------------------------\n",
      "-not the hacking and gagging and spitting part  please\n",
      "-okay then how about we try out some french cuisine  saturday  night\n",
      "-------------------------------------\n",
      "-you are asking me out  that is so cute what is your name again\n",
      "-forget it\n",
      "-------------------------------------\n",
      "-no no it is my fault  we did not have a proper introduction \n",
      "-cameron\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(\"-\"+str(clean_q[i]))\n",
    "    print(\"-\"+str(clean_a[i]))\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HZVOE7LUDp86"
   },
   "source": [
    "# Upper and lower boundaries for sentence length.\n",
    "I will do a small analysis on the length of the sentence. I would like to include sentences that will have a meaning for the whole process,thus i will see by finding the percentile values the upper and lower boundaries for my dataset. If we set the value of the length of the sentence to be from 2 words( not much meaning in sentence with one word) and 20 words,we will still include a large portion of the dataset,as it can easily be seen from the percentile values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40974,
     "status": "ok",
     "timestamp": 1546602636456,
     "user": {
      "displayName": "Ioannis Avgeros",
      "photoUrl": "",
      "userId": "10653338927563797423"
     },
     "user_tz": -120
    },
    "id": "V9T6uXuNDvqQ",
    "outputId": "3d7d5a9c-473e-483b-baed-c4f018243272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th percentile value:0.0\n",
      "25th percentile value:4.0\n",
      "50th percentile value:7.0\n",
      "80th percentile value:16.0\n",
      "85th percentile value:19.0\n",
      "90th percentile value:24.0\n",
      "95th percentile value:32.0\n",
      "99th percentile value:58.0\n"
     ]
    }
   ],
   "source": [
    "# Find the length of sentences\n",
    "lengths = []\n",
    "for question in clean_q:\n",
    "    lengths.append(len(question.split()))\n",
    "for answer in clean_a:\n",
    "    lengths.append(len(answer.split()))\n",
    "\n",
    "# Create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])\n",
    "lengths.describe()\n",
    "print('0th percentile value:'+str(np.percentile(lengths, 0)))\n",
    "print('25th percentile value:'+str(np.percentile(lengths, 25)))\n",
    "print('50th percentile value:'+str(np.percentile(lengths, 50)))\n",
    "print('80th percentile value:'+str(np.percentile(lengths, 80)))\n",
    "print('85th percentile value:'+str(np.percentile(lengths, 85)))\n",
    "print('90th percentile value:'+str(np.percentile(lengths, 90)))\n",
    "print('95th percentile value:'+str(np.percentile(lengths, 95)))\n",
    "print('99th percentile value:'+str(np.percentile(lengths, 99)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J7lfnmR3D9hs"
   },
   "source": [
    "# Preprocess\n",
    "I must filter the dataset. Also, a dictionary must be created containing the words and a numerical value associated with them. Neural networks work with numbers,not strings.\n",
    "Furthermore, I decided to  force the following rules:\n",
    "\n",
    "\n",
    "1.  For a word to be included,it must be present at least 10 times.\n",
    "2.  Sentences that the percentage of uknown words is greater than 33 percent,will not be included in the train dataset.\n",
    "\n",
    "I also included four keywords to help me out with the process:\n",
    "\n",
    "\n",
    "*   PAD:  The sentences are of a fixed size. In order to make all the sentences to be equal size,we replace the sentences with far less length than the max we decided to follow, with this keyword\n",
    "* EOS: End of stream,for the answer sequence\n",
    "* GO : Start of answer sequence\n",
    "* UNK: Used for word in sentences not present in vocabulary.}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "skxLFOtYkyqD"
   },
   "outputs": [],
   "source": [
    "def return_valid_sentences(clean_questions,clean_answers,min_,max_):\n",
    "    questions_valid_=[]\n",
    "    answers_valid_ = []\n",
    "    questions_valid = []\n",
    "    answers_valid = []\n",
    "\n",
    "    for i,question in enumerate(clean_questions):\n",
    "        if len(question.split()) >= min_ and len(question.split()) <= max_:\n",
    "            questions_valid_.append(question)\n",
    "            answers_valid_.append(clean_answers[i])\n",
    "\n",
    "    for i,answer in enumerate(answers_valid_):\n",
    "        if len(answer.split()) >= min_ and len(answer.split()) <= max_:\n",
    "            answers_valid.append(answer)\n",
    "            questions_valid.append(questions_valid_[i])\n",
    "    return (answers_valid,questions_valid)\n",
    "\n",
    "def get_vocab(av,qv):\n",
    "    vocab = {}\n",
    "    for question in qv:\n",
    "        for word in question.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 1\n",
    "            else:\n",
    "                vocab[word] += 1\n",
    "\n",
    "    for answer in av:\n",
    "        for word in answer.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 1\n",
    "            else:\n",
    "                vocab[word] += 1\n",
    "    return vocab\n",
    "\n",
    "def remove_infrequent_words(threshold,vocab):\n",
    "    q = {}\n",
    "    word_num=1\n",
    "    for word, count in vocab.items():\n",
    "        if count >= threshold:\n",
    "            q[word] = word_num\n",
    "            word_num+=1\n",
    "    return q\n",
    "\n",
    "def text_to_int(q,a,voc):\n",
    "    q_int = []\n",
    "    for question in q:\n",
    "        ints = []\n",
    "        for word in question.split():\n",
    "            if word not in voc:\n",
    "                ints.append(voc['<UNK>'])\n",
    "            else:\n",
    "                ints.append(voc[word])\n",
    "        q_int.append(ints)\n",
    "\n",
    "    a_int = []\n",
    "    for answer in a:\n",
    "        ints = []\n",
    "        for word in answer.split():\n",
    "            if word not in voc:\n",
    "                ints.append(voc['<UNK>'])\n",
    "            else:\n",
    "                ints.append(voc[word])\n",
    "        a_int.append(ints)\n",
    "    return (q_int,a_int)\n",
    "\n",
    "def remove_uninteresting(q,a,voc):\n",
    "    qr=[]\n",
    "    ar=[]\n",
    "    art=[]\n",
    "    qrt=[]\n",
    "\n",
    "    for i,q_ in enumerate(q):\n",
    "        cnt=0\n",
    "        tmp=q_.split(' ')\n",
    "        for x in tmp:\n",
    "            if x!='' and x not in voc.keys():\n",
    "                cnt+=1\n",
    "        if cnt<round(len(tmp)/3):\n",
    "            qrt.append(q_)\n",
    "            art.append(a[i])\n",
    "    for i,a_ in enumerate(art):\n",
    "        cnt=0\n",
    "        tmp=a_.split(' ')\n",
    "        for x in tmp:\n",
    "            if x!='' and x not in voc.keys():\n",
    "                cnt+=1\n",
    "        if cnt<round(len(tmp)/3):\n",
    "            ar.append(a_)\n",
    "            qr.append(qrt[i])\n",
    "    return (qr,ar)\n",
    "  \n",
    "def pad_sentence(s,code,max_):\n",
    "  max_sentence = max_\n",
    "  return [sentence + [code] * (max_sentence - len(sentence)) for sentence in s]\n",
    "\n",
    "def get_mask(target_seqs):\n",
    "  masked=[]\n",
    "  for s in target_seqs:\n",
    "    tmp=[]\n",
    "    for x in s:\n",
    "      if x==0:\n",
    "        tmp.append(0)\n",
    "      else:\n",
    "        tmp.append(1)\n",
    "    masked.append(tmp)\n",
    "  return masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40937,
     "status": "ok",
     "timestamp": 1546602639994,
     "user": {
      "displayName": "Ioannis Avgeros",
      "photoUrl": "",
      "userId": "10653338927563797423"
     },
     "user_tz": -120
    },
    "id": "3o2KSz-Vk2yr",
    "outputId": "9453b42d-6504-45fc-b6e6-c02d0a7941e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138333\n",
      "126329\n",
      "126329\n",
      "8102\n",
      "8103\n",
      "8104\n",
      "[1, 2, 3, 4, 5, 6, 7, 8103, 8, 9, 10, 11, 7, 12] \n",
      " well i thought we would start with pronunciation if that is okay with you\n",
      "====================================================\n",
      "[13, 14, 8103, 15, 8103, 15, 16, 17, 18] \n",
      " not the hacking and gagging and spitting part  please\n",
      "126329 126329\n"
     ]
    }
   ],
   "source": [
    "min_=2\n",
    "max_=20\n",
    "(a_valid,q_valid)=return_valid_sentences(clean_q,clean_a,min_,max_)\n",
    "vocab=get_vocab(a_valid,q_valid)\n",
    "threshold=10\n",
    "word2idx=remove_infrequent_words(threshold,vocab)\n",
    "print(len(q_valid))\n",
    "(q_valid,a_valid)=remove_uninteresting(q_valid,a_valid,word2idx)\n",
    "print(len(q_valid))\n",
    "print(len(a_valid))\n",
    "\n",
    "codes = ['<EOS>','<UNK>','<GO>']\n",
    "word2idx['<PAD>']=0\n",
    "for code in codes:\n",
    "    word2idx[code] = len(word2idx)\n",
    "    print(word2idx[code])\n",
    "idx2word = {v_i: v for v, v_i in word2idx.items()}\n",
    "    \n",
    "(q_int,a_int)=text_to_int(q_valid,a_valid,word2idx)\n",
    "\n",
    "print(q_int[0],'\\n',q_valid[0])\n",
    "print('====================================================')\n",
    "print(a_int[0],'\\n',a_valid[0])\n",
    "input_length=len(q_int)\n",
    "print(input_length,len(a_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6V2YeW-4jcki"
   },
   "outputs": [],
   "source": [
    "unk_id = word2idx['<UNK>']   # 1\n",
    "pad_id = word2idx['<PAD>']     # 0\n",
    "\n",
    "start_id = word2idx['<GO>']  # 8002\n",
    "end_id =  word2idx['<EOS>']  # 8003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w9miSWGEGuKZ"
   },
   "source": [
    "# Model creation\n",
    "Main part of the procedure.\n",
    "We use the seq to seq function out of tensorflow.\n",
    "Sequence to sequence is a neural network architecture. It contain two basically different neural networks, the encoder and the decoder, and its purpose is to feed the encoder with a sequence,the encoder will try to find a different represantation,which afterwards will be forwarded to the decoder,whose job is to decode the encoded sequence and produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uwg7cOYwzsmP"
   },
   "outputs": [],
   "source": [
    "def create_model(encode_seqs, decode_seqs, src_vocab_size, emb_dim, is_train=True, reuse=False):\n",
    "    with tf.variable_scope(\"model\", reuse=reuse):\n",
    "        with tf.variable_scope(\"embedding\") as vs:\n",
    "            #The input layer:\n",
    "            net_encode = EmbeddingInputlayer(\n",
    "                inputs = encode_seqs,\n",
    "                vocabulary_size = src_vocab_size,\n",
    "                embedding_size = emb_dim,\n",
    "                name = 'seq_embedding')\n",
    "            vs.reuse_variables()\n",
    "            #The output layer\n",
    "            net_decode = EmbeddingInputlayer(\n",
    "                inputs = decode_seqs,\n",
    "                vocabulary_size = src_vocab_size,\n",
    "                embedding_size = emb_dim,\n",
    "                name = 'seq_embedding')\n",
    "            \n",
    "        net_rnn = Seq2Seq(net_encode, net_decode,\n",
    "                cell_fn = tf.nn.rnn_cell.LSTMCell,\n",
    "                n_hidden = emb_dim,\n",
    "                initializer = tf.random_uniform_initializer(-0.1, 0.1),\n",
    "                encode_sequence_length = retrieve_seq_length_op2(encode_seqs),\n",
    "                decode_sequence_length = retrieve_seq_length_op2(decode_seqs),\n",
    "                initial_state_encode = None,\n",
    "                dropout = (0.5 if is_train else None),\n",
    "                n_layer = 3,\n",
    "                return_seq_2d = True,\n",
    "                name = 'seq2seq')\n",
    "\n",
    "        net_out = DenseLayer(net_rnn, n_units=src_vocab_size, act=tf.identity, name='output')\n",
    "    return net_out, net_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HwGSMTTlLpjQ"
   },
   "source": [
    "Reset the graph in case something from a previous execution remains. Also,initialize variables for configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NbpZEY-Vnyt4"
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "sess_config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GXo4FI-SOtWS"
   },
   "source": [
    " \n",
    "Here I set the placeholders for input and output.We also want to have a third placeholder,the mask.I use the cross entropy loss function ( cross-entropy describes the loss between two probability distributions and measures how close is the predicted distribution to the true distribution), alongside mask.Mask is  a vector containing 1's where the sentence have a word and 0's when it has PAD item.The mask is going to be used to skip any input with mask 0 by copying the previous hidden state of the cell; it will proceed normally for any input with mask 1.Masking allows us to handle various length inputs in RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35873,
     "status": "ok",
     "timestamp": 1546602645515,
     "user": {
      "displayName": "Ioannis Avgeros",
      "photoUrl": "",
      "userId": "10653338927563797423"
     },
     "user_tz": -120
    },
    "id": "zpq_OUaMxmm_",
    "outputId": "75dd8f92-0733-47ab-b789-9afd20eb7330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] EmbeddingInputlayer model/embedding/seq_embedding: (8105, 512)\n",
      "[TL] EmbeddingInputlayer model/embedding/seq_embedding: (8105, 512)\n",
      "[TL] [*] Seq2Seq model/seq2seq: n_hidden: 512 cell_fn: LSTMCell dropout: 0.5 n_layer: 3\n",
      "[TL] DynamicRNNLayer model/seq2seq/encode: n_hidden: 512, in_dim: 3 in_shape: (128, ?, 512) cell_fn: LSTMCell dropout: 0.5 n_layer: 3\n",
      "[TL]        batch_size (concurrent processes): 128\n",
      "[TL] DynamicRNNLayer model/seq2seq/decode: n_hidden: 512, in_dim: 3 in_shape: (128, ?, 512) cell_fn: LSTMCell dropout: 0.5 n_layer: 3\n",
      "[TL]        batch_size (concurrent processes): 128\n",
      "[TL] DenseLayer  model/output: 8105 No Activation\n",
      "[TL]   param   0: model/embedding/seq_embedding/embeddings:0 (8105, 512)        float32_ref\n",
      "[TL]   param   1: model/seq2seq/encode/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0 (1024, 2048)       float32_ref\n",
      "[TL]   param   2: model/seq2seq/encode/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0 (2048,)            float32_ref\n",
      "[TL]   param   3: model/seq2seq/encode/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0 (1024, 2048)       float32_ref\n",
      "[TL]   param   4: model/seq2seq/encode/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0 (2048,)            float32_ref\n",
      "[TL]   param   5: model/seq2seq/encode/rnn/multi_rnn_cell/cell_2/lstm_cell/kernel:0 (1024, 2048)       float32_ref\n",
      "[TL]   param   6: model/seq2seq/encode/rnn/multi_rnn_cell/cell_2/lstm_cell/bias:0 (2048,)            float32_ref\n",
      "[TL]   param   7: model/seq2seq/decode/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0 (1024, 2048)       float32_ref\n",
      "[TL]   param   8: model/seq2seq/decode/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0 (2048,)            float32_ref\n",
      "[TL]   param   9: model/seq2seq/decode/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0 (1024, 2048)       float32_ref\n",
      "[TL]   param  10: model/seq2seq/decode/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0 (2048,)            float32_ref\n",
      "[TL]   param  11: model/seq2seq/decode/rnn/multi_rnn_cell/cell_2/lstm_cell/kernel:0 (1024, 2048)       float32_ref\n",
      "[TL]   param  12: model/seq2seq/decode/rnn/multi_rnn_cell/cell_2/lstm_cell/bias:0 (2048,)            float32_ref\n",
      "[TL]   param  13: model/output/W:0     (512, 8105)        float32_ref\n",
      "[TL]   param  14: model/output/b:0     (8105,)            float32_ref\n",
      "[TL]   num of params: 20902825\n",
      "[TL] EmbeddingInputlayer model/embedding/seq_embedding: (8105, 512)\n",
      "[TL] EmbeddingInputlayer model/embedding/seq_embedding: (8105, 512)\n",
      "[TL] [*] Seq2Seq model/seq2seq: n_hidden: 512 cell_fn: LSTMCell dropout: None n_layer: 3\n",
      "[TL] DynamicRNNLayer model/seq2seq/encode: n_hidden: 512, in_dim: 3 in_shape: (1, ?, 512) cell_fn: LSTMCell dropout: None n_layer: 3\n",
      "[TL]        batch_size (concurrent processes): 1\n",
      "[TL] DynamicRNNLayer model/seq2seq/decode: n_hidden: 512, in_dim: 3 in_shape: (1, ?, 512) cell_fn: LSTMCell dropout: None n_layer: 3\n",
      "[TL]        batch_size (concurrent processes): 1\n",
      "[TL] DenseLayer  model/output: 8105 No Activation\n"
     ]
    }
   ],
   "source": [
    "batch_size=128\n",
    "n_step = len(q_int) // batch_size\n",
    "with tf.device('/device:GPU:0'):\n",
    "  encode_seqs_= tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"encode_seqs\")\n",
    "  decode_seqs_ = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"decode_seqs\")\n",
    "  target_seqs_ = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_seqs\")\n",
    "  target_mask_ = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_mask\") \n",
    "  net_out, _ = create_model(encode_seqs_, decode_seqs_, len(word2idx), 512, is_train=True, reuse=False)\n",
    "  net_out.print_params(False)\n",
    "\n",
    "  # Inference Data Placeholders\n",
    "  encode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"encode_seqs\")\n",
    "  decode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"decode_seqs\")\n",
    "\n",
    "  net, net_rnn = create_model(encode_seqs2, decode_seqs2, len(word2idx), 512, is_train=False, reuse=True)\n",
    "  y = tf.nn.softmax(net.outputs)\n",
    "\n",
    "  # Loss Function\n",
    "  loss = tl.cost.cross_entropy_seq_with_mask(logits=net_out.outputs, target_seqs=target_seqs_, \n",
    "                                              input_mask=target_mask_, return_details=False, name='cost')\n",
    "\n",
    "  # Optimizer\n",
    "  optimizer=tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "  train_op = optimizer.minimize(loss)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPhgKa546MuI"
   },
   "source": [
    "Here we initialize the variables and initialize a session with the beforementioned configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1546602710091,
     "user": {
      "displayName": "Ioannis Avgeros",
      "photoUrl": "",
      "userId": "10653338927563797423"
     },
     "user_tz": -120
    },
    "id": "RILqWCzi3j6v",
    "outputId": "4426cded-3c73-4bc4-f841-2525d6ee49e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "a=tf.initialize_all_variables()\n",
    "b=tf.global_variables_initializer()\n",
    "sess = tf.Session(config=sess_config)\n",
    "\n",
    "# Actually intialize the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9_toueGy9gs_"
   },
   "source": [
    "Inference is the answer to a question made by the user.What it does is it gets the question string and inserts it into the neural network designed specifically for user questions. It uses the trained model in the following step to generate the most appropriate sequence,based on the knowledge gained from the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8aaUZfvRzBKi"
   },
   "outputs": [],
   "source": [
    "def inference(seed):\n",
    "  seed=clean_text(seed)\n",
    "  seed_id = [word2idx.get(w, unk_id) for w in seed.split(\" \")]\n",
    "  \n",
    "  state = sess.run(net_rnn.final_state_encode,\n",
    "                  {encode_seqs2: [seed_id]})\n",
    "  #Run the softmax function to the output of the decoded rnn used for inference\n",
    "  o, state = sess.run([y, net_rnn.final_state_decode],\n",
    "                  {net_rnn.initial_state_decode: state,\n",
    "                  decode_seqs2: [[start_id]]})\n",
    "  w_id = tl.nlp.sample_top(o[0], top_k=10)\n",
    "  w = idx2word[w_id]\n",
    "  # Decode and feed state iteratively\n",
    "  sentence = [w]\n",
    "  #As far as the output symbol is not EOS,keep feeding.\n",
    "  for _ in range(30): # max sentence length\n",
    "      o, state = sess.run([y, net_rnn.final_state_decode],\n",
    "                      {net_rnn.initial_state_decode: state,\n",
    "                      decode_seqs2: [[w_id]]})\n",
    "      w_id = tl.nlp.sample_top(o[0], top_k=10)\n",
    "      w = idx2word[w_id]\n",
    "      if w_id == end_id:\n",
    "          break\n",
    "      sentence = sentence + [w]\n",
    "  return sentence\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OUVKeZ8lAlgX"
   },
   "source": [
    "The main training phase. The iteration iterates up to number of epochs defined below, and splits dataset to trainX and trainY.Before we split the dataset,we shuffle the data to assure at most that batches will contain data with multiple distributions and the neural network will be trained with as many different data samples as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hZ3PLCCq1ZPw"
   },
   "outputs": [],
   "source": [
    "seeds = [\"do you love trump\",\"how are you\",\"i love you\",\"fuck you\",\"do you like drinking\",\"what is your name\"]\n",
    "num_epochs=25\n",
    "sess.run(a)\n",
    "sess.run(b)\n",
    "import sys\n",
    "sys.stdout.flush()\n",
    "for epoch in range(num_epochs):\n",
    "    trainX, trainY = shuffle(q_int, a_int, random_state=0)\n",
    "    total_loss, n_iter = 0, 0\n",
    "    for X, Y in tqdm(tl.iterate.minibatches(inputs=trainX, targets=trainY, batch_size=batch_size, shuffle=False), \n",
    "                    total=n_step, desc='Epoch[{}/{}]'.format(epoch + 1, num_epochs), leave=False):\n",
    "\n",
    "        X = pad_sentence(X,word2idx['<PAD>'],max_)\n",
    "        _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=end_id)\n",
    "        _target_seqs = pad_sentence(_target_seqs,word2idx['<PAD>'],max_+1)\n",
    "        _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=start_id, remove_last=False)\n",
    "        _decode_seqs = pad_sentence(_decode_seqs,word2idx['<PAD>'],max_+1)\n",
    "        _target_mask = get_mask(_target_seqs)\n",
    "        \n",
    "        _, loss_iter = sess.run([train_op, loss], {encode_seqs_: X, decode_seqs_: _decode_seqs,\n",
    "                        target_seqs_: _target_seqs, target_mask_: _target_mask})\n",
    "        total_loss += loss_iter\n",
    "        n_iter += 1\n",
    "\n",
    "    # printing average loss after every epoch\n",
    "    print('Epoch [{}/{}]: loss {:.4f} learning rate={:.4f}'.format(epoch + 1, num_epochs, total_loss / n_iter,optimizer._lr))\n",
    "\n",
    "    # inference after every epoch\n",
    "    for seed in seeds:\n",
    "        print(\"Query >\", seed)\n",
    "        for _ in range(5):\n",
    "            sentence = inference(seed)\n",
    "            print(\" >\", ' '.join(sentence))\n",
    "\n",
    "    # saving the model\n",
    "    tl.files.save_npz(net.all_params, name='model.npz', sess=sess)\n",
    "from google.colab import files\n",
    "files.download('model.npz')\n",
    "# session cleanup\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVSomRDhORfT"
   },
   "source": [
    "After the lengthy training procedure,we save and keep the model to use it again without having to train the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2110,
     "status": "ok",
     "timestamp": 1546610199221,
     "user": {
      "displayName": "Ioannis Avgeros",
      "photoUrl": "",
      "userId": "10653338927563797423"
     },
     "user_tz": -120
    },
    "id": "DI-IyYADLFIc",
    "outputId": "0508489a-7605-4c1e-df0b-1345c147899e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] [*] Load model.npz SUCCESS!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorlayer.layers.dense.base_dense.DenseLayer at 0x7fc7e857ae10>"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.files.load_and_assign_npz(sess=sess, name='model.npz', network=net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3H_-NNpAOcW4"
   },
   "source": [
    "Interface to communicate with user.q exits the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1388
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199704,
     "status": "error",
     "timestamp": 1546610492488,
     "user": {
      "displayName": "Ioannis Avgeros",
      "photoUrl": "",
      "userId": "10653338927563797423"
     },
     "user_tz": -120
    },
    "id": "yhpoVGHRayfl",
    "outputId": "bbfee372-4f1a-48cf-f9f0-f15374ebdfdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q to exit\n",
      "-my heart hurts\n",
      "- i am sorry\n",
      "-i feel negligible\n",
      "- and the next time you are going to get a lot more\n",
      "-i feel empty\n",
      "- well what happened\n",
      "-i lost my mother\n",
      "- what is it\n",
      "-the person who gave birth to me\n",
      "- you will have to <UNK> it up\n",
      "-suck ?\n",
      "- i will be back soon\n",
      "-why you always leving\n",
      "- i am an actor\n",
      "-what do you do\n",
      "- i am going to kill her\n",
      "-Who?\n",
      "- your wife she is a <UNK>\n",
      "-What the hell\n",
      "- what are you talking about\n",
      "-Robots really tok over\n",
      "- and what are you thinking about\n",
      "-stopping you all\n",
      "- and you are the one who wants to be a fireman\n",
      "-i never said that\n",
      "- i have heard what i wanted\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-1de9b9f8352a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'q to exit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m'q'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m'q'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "s=''\n",
    "print('q to exit')\n",
    "while s!='q':\n",
    "  s=input('-')\n",
    "  if s!='q':\n",
    "    m=inference(s)\n",
    "    print(\"-\",' '.join(m))\n",
    "print(\"Adios!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EP5SmeZDJ6mY"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jksG8PRYKbqp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled3.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
